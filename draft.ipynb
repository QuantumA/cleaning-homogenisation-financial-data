{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import datetime as dt\n",
    "from time import mktime\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from typing import List\n",
    "import re\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_indices = [\"%5EIBEX\", \"%5EBFX\",\"%5EBVSP\", \"%5EDJI\", \"%5EFCHI\", \"%5EFTSE\", \"%5EGDAXI\", \"%5EHSI\", \"%5EIBEX\", \n",
    "                \"%5EMXX\", \"%5EJKSE\", \"%5EMERV\", \"%5EOMXSPI\", \"%5EOSEAX\", \"%5ESSMI\", \"%5ESTI\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_unix(date):\n",
    "    \"\"\"\n",
    "    converts date to unix timestamp\n",
    "    \n",
    "    parameters: date - in format (dd-mm-yyyy)\n",
    "    \n",
    "    returns integer unix timestamp\n",
    "    \"\"\"\n",
    "    datum = dt.datetime.strptime(date, '%d-%m-%Y')\n",
    "    \n",
    "    return int(mktime(datum.timetuple()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_crumbs_and_cookies(ticker: str):\n",
    "    # Thanks to MAIK ROSENHEINRICH\n",
    "    \"\"\"\n",
    "    get crumb and cookies for historical data csv download from yahoo finance  \n",
    "    parameters: stock - short-handle identifier of the company    \n",
    "    returns a tuple of header, crumb and cookie\n",
    "    \"\"\"   \n",
    "    url = 'https://finance.yahoo.com/quote/{}/history'.format(ticker)\n",
    "    \n",
    "    with requests.session():\n",
    "        header = {'Connection': 'keep-alive',\n",
    "                   'Expires': '-1',\n",
    "                   'Upgrade-Insecure-Requests': '1',\n",
    "                   'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) \\\n",
    "                   AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.99 Safari/537.36'\n",
    "                   }        \n",
    "        website = requests.get(url, headers=header)\n",
    "        soup = BeautifulSoup(website.text, 'lxml')\n",
    "        \n",
    "        crumb = re.findall('\"CrumbStore\":{\"crumb\":\"(.+?)\"}', str(soup))\n",
    "        output=(header, crumb[0], website.cookies)\n",
    "        return output   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_begin_unix = convert_to_unix(\"01-12-2018\")\n",
    "day_end_unix = convert_to_unix(\"21-09-2020\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "header, crumb, cookies = get_crumbs_and_cookies(lista_indices[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "with requests.session():\n",
    "        \n",
    "            url = 'https://query1.finance.yahoo.com/v7/finance/download/' \\\n",
    "                '{stock}?period1={day_begin}&period2={day_end}&interval={interval}&events=history&crumb={crumb}' \\\n",
    "                .format(stock=lista_indices[1], day_begin=day_begin_unix, day_end=day_end_unix, interval='1d', crumb=crumb)\n",
    "                \n",
    "            website = requests.get(url, headers=header, cookies=cookies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_historical_data(tickers: List[str], day_begin: str, day_end: str, interval='1d'):\n",
    "\n",
    "    historical_prices = None\n",
    "    df_create = False\n",
    "\n",
    "\n",
    "    for ticker in tickers:\n",
    "        error1='404 Not Found: Timestamp data missing.' \n",
    "    \n",
    "        day_begin_unix = convert_to_unix(day_begin)\n",
    "        day_end_unix = convert_to_unix(day_end)   \n",
    "    \n",
    "        header, crumb, cookies = get_crumbs_and_cookies(ticker)\n",
    "    \n",
    "        with requests.session():\n",
    "        \n",
    "            url = 'https://query1.finance.yahoo.com/v7/finance/download/' \\\n",
    "                '{stock}?period1={day_begin}&period2={day_end}&interval={interval}&events=history&crumb={crumb}' \\\n",
    "                .format(stock=ticker, day_begin=day_begin_unix, day_end=day_end_unix, interval=interval, crumb=crumb)\n",
    "                \n",
    "            website = requests.get(url, headers=header, cookies=cookies)\n",
    "            if website.status_code == 200:\n",
    "                if not df_create:\n",
    "                    historical_prices = pd.read_csv(io.StringIO(website.text))\n",
    "                    historical_prices[\"ticker\"] = ticker\n",
    "                    df_create = True\n",
    "                else:\n",
    "                    temp_df = pd.read_csv(io.StringIO(website.text))\n",
    "                    temp_df[\"ticker\"] = ticker\n",
    "                    historical_prices = historical_prices.append(temp_df, ignore_index=True)\n",
    "                \n",
    "                print(\"Obteniendo histórico para {}\".format(ticker))\n",
    "                print(\"longitud del df: {}\".format(len(historical_prices.Date)))\n",
    "            \n",
    "            else:\n",
    "                print(\"Error downloading data from ticker {}\".format(ticker))\n",
    "                print(\"Response was {}\".format(website.text))\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "          # make request.\n",
    "          # read df.\n",
    "          # save each df into a single csv.\n",
    "\n",
    "    return historical_prices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_currency(tickers: List[str]):\n",
    "    \n",
    "    indice_divisa = []\n",
    "\n",
    "    for ticker in tickers:\n",
    "\n",
    "        header, crumb, cookies = get_crumbs_and_cookies(ticker)\n",
    "            \n",
    "        with requests.session():\n",
    "                \n",
    "            url = \"https://es.finance.yahoo.com/quote/\"+ticker+\"/components/\"            \n",
    "            website = requests.get(url, headers=header, cookies=cookies)\n",
    "\n",
    "        soup = BeautifulSoup(website.text)\n",
    "        divisa = re.findall('Divisa en [a-zA-Z]{3}', str(soup))\n",
    "        divisa = divisa[0][len(divisa[0])-3:]\n",
    "        indice_divisa.append((ticker, divisa))\n",
    "\n",
    "    return indice_divisa\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_components(tickers=List[str]):\n",
    "\n",
    "    componentes = {}\n",
    "\n",
    "    for ticker in tickers:\n",
    "\n",
    "        header, crumb, cookies = get_crumbs_and_cookies(ticker)\n",
    "                    \n",
    "        with requests.session():\n",
    "                        \n",
    "            intentos = 3\n",
    "            while intentos > 0:\n",
    "                url = \"https://es.finance.yahoo.com/quote/\"+ticker+\"/components/\"            \n",
    "                website = requests.get(url, headers=header, cookies=cookies)\n",
    "\n",
    "                print(\"Obteniendo componentes de {}\".format(ticker))\n",
    "                if website.status_code == 200:\n",
    "\n",
    "                    soup = BeautifulSoup(website.text)\n",
    "                    try:\n",
    "                        df = pd.read_html(str(soup))[0]\n",
    "                        df = df.iloc[:,[0,1]]\n",
    "                        componentes[ticker] = df.to_dict()\n",
    "\n",
    "                    except ValueError as ve:\n",
    "                        print(ve)\n",
    "                        print(url)\n",
    "                        next\n",
    "                    intentos = 0\n",
    "                else:\n",
    "                    print(\"Error al conectar con yahoo, intentos restantes: {}\".format(intentos))\n",
    "                    intentos -= 1\n",
    "                    if intentos == 0:\n",
    "                        print(\"No se ha conseguido descargar información para el ticker: {}\".format(ticker))\n",
    "\n",
    "\n",
    "        \n",
    "    return componentes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_html(str(soup))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obteniendo componentes de %5EIBEX\n",
      "Obteniendo componentes de %5EBFX\n",
      "Obteniendo componentes de %5EBVSP\n",
      "Obteniendo componentes de %5EDJI\n",
      "No tables found\n",
      "https://es.finance.yahoo.com/quote/%5EDJI/components/\n",
      "Obteniendo componentes de %5EFCHI\n",
      "Obteniendo componentes de %5EFTSE\n",
      "Obteniendo componentes de %5EGDAXI\n",
      "Obteniendo componentes de %5EHSI\n",
      "Obteniendo componentes de %5EIBEX\n",
      "Obteniendo componentes de %5EMXX\n",
      "Obteniendo componentes de %5EJKSE\n",
      "Obteniendo componentes de %5EMERV\n",
      "Obteniendo componentes de %5EOMXSPI\n",
      "Obteniendo componentes de %5EOSEAX\n",
      "Obteniendo componentes de %5ESSMI\n",
      "Obteniendo componentes de %5ESTI\n"
     ]
    }
   ],
   "source": [
    "c = get_components(lista_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['%5EIBEX', '%5EBFX', '%5EBVSP', '%5EDJI', '%5EFCHI', '%5EFTSE', '%5EGDAXI', '%5EHSI', '%5EMXX', '%5EJKSE', '%5EMERV', '%5EOMXSPI', '%5EOSEAX', '%5ESSMI', '%5ESTI'])"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_historical_data(lista_indices, \"01-01-2018\", \"01-01-2020\", interval='1d')\n",
    "divisas = get_currency(lista_indices)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4b93c986a772c3d915d9a0c0155b758281b32f4512146cda581c6cbe9e806c15"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
